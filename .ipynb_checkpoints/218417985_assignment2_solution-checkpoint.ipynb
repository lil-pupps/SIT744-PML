{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### <span style=\"color:#0b486b\">SIT744 Practical Machine Learning for Data Science</span>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:#0b486b\">Assignment Two: Deep Neural Networks, Representation Learning, and Text Analytics</span>\n",
    "### Due: <span style=\"color:red\">24:00pm 24 September 2019</span>  (Tuesday)\n",
    "\n",
    "#### <span style=\"color:red\">Important note:</span> This is an **individual** assignment. It contributes **40%** to your final mark. Read the assignment instruction carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been prepared for your to complete Assignment 2. The theme of this assignment is about practical machine learning knowledge and skills in deep neural networks, word embedding and text analytics. Some sections have been partially completed to help you get\n",
    "started. **The total marks for this notebook are 80 marks, which will be re-scaled to 40 marks in the grade.**.\n",
    "\n",
    "* Before you start, read the entire notebook carefully once to understand what you need to do. <br><br>\n",
    "\n",
    "* For each cell marked with **#YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL**, there will be places where you **must** supply your own codes when instructed. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Instruction</span>\n",
    "\n",
    "This assignment contains **two** parts \n",
    "\n",
    "* Part 1: Deep Feedforward Neural Network **[45 points]**\n",
    "* Part 2: Word2Vec, text analytics and application **[35 points]**\n",
    "\n",
    "**Hint**: This assignment was essentially designed based on the lectures and practical lab sessions covered from Week 5 to 9. You are strongly recommended to go through these contents thoroughly which might help you to complete this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">What to submit</span>\n",
    "\n",
    "This assignment is to be completed individually and submitted to CloudDeakin. **By the due date, you are required to submit the following files to the corresponding Assignment in CloudDeakin**:\n",
    "\n",
    "1.\t<span style=\"color:red\">**[YourID]_assignment2_solution.ipynp**</span>:  **this is your Python notebook solution source file**.\n",
    "1.\t<span style=\"color:red\">**[YourID]_assingment2_output.html**</span>: **this is the output of your Python notebook solution *exported* in html format**.\n",
    "1.\t<span style=\"color:red\">Any extra files needed to complete your assignment</span> (e.g., images used in your answers).\n",
    "\n",
    "For example, if your student ID is: 123456, you will then need to submit the following files:\n",
    "* 123456_assignment2_solution.ipynp\n",
    "* 123456_assignment2_output.html\n",
    "* any extra files or subfolder you might have (this can be named according to your preference).\n",
    "\n",
    "<span style=\"color:red\">Please proceed to the content below to complete your assignment!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: Deep Feedforward Neural Network </span>\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[Total mark for this part: 45 points]**<span></div>\n",
    "\n",
    "The first part of this assignment is for you to demonstrate the knowledge in deep learning that you have acquired from the lectures and practical lab materials. Most of the contents in this assignment are drawn from the practical materials in week 5, 6 and 7 for deep neural networks. Going through these materials before attempting this assignment is highly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run the following cell to create necessary subfolders for this assignment. You must **not** modify these codes and **must** run it first*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary subfolders to store immediate files for this assignment.\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"./models/dnn0\"):\n",
    "    os.makedirs(\"models/dnn0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this assignment is to apply DNN to recognize letters from A-Z. You have played with MNIST dataset in your pracs and this should have given a good sense of how to apply DNN on images for recognition task. \n",
    "\n",
    "In this assignment, you are going to work with the **notMNIST** dataset for *letter recognition task*. The dataset contains 10 classes of letters A-J taken from different fonts. You will see some examples at the visualization task in the next part. A short blog about the data can be found [here](http://yaroslavvb.blogspot.com.au/2011/09/notmnist-dataset.html).\n",
    "\n",
    "Here we only consider a small subset which can be found at [this link](http://yaroslavvb.com/upload/notMNIST/notMNIST_small.mat). This file has been already downloaded and stored in subfolder `datasets` of this assignment folder. The file is in *Matlab* format, thus our first task is to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 1.1**</span>. Load the data into *`numpy array`* format of two variables:\n",
    "* *`x`*: storing features with dimension `[num_samples, width, height]` (`num_samples`: number of samples, `width`: image width, `height`: image height), and\n",
    "* *`y`*: storing labels with dimension `num_samples`. \n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[3 points]**</span></div>\n",
    "\n",
    "Enter the missing codes in the following cell to complete this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'images', 'labels'])\n",
      "(18724,)\n"
     ]
    }
   ],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "data = sio.matlab.loadmat(\"datasets/notMNIST_small.mat\")\n",
    "#print(type(data))\n",
    "#print(data.images)\n",
    "#print(data['images'])\n",
    "print(data.keys())\n",
    "x, y = data['images'],data['labels']\n",
    "x = np.rollaxis(x, axis=2)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 1.2**</span>. Print out the total number of data points, and the *unique* labels in this dataset.\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[3 points]**</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "#YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "data_points = x[1].reshape(28*28,)\n",
    "print(data_points.shape)\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 1.3**</span>. Display 100  images in the form of `10x10` matrix, each row showing 10 *random* images of a label. You might decide to use the function `display_images` provided at the beginning of this assignment, or you can write your own codes.\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[4 points]**</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is a utility to display images from the dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def display_images(images, shape):\n",
    "    fig = plt.figure(figsize=shape)\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "    for i in range(np.prod(shape)):\n",
    "        p = fig.add_subplot(shape[0], shape[1], i+1, xticks=[], yticks=[])\n",
    "        p.imshow(images[i], cmap=plt.cm.bone)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "\n",
    "unique_labels = np.unique(y)\n",
    "images = []\n",
    "for l in unique_labels:    \n",
    "    idx = np.where(y == l)[0]\n",
    "    idx = idx[np.random.permutation(len(idx))[:10]]    \n",
    "    for i in idx:\n",
    "        images += [x[i]]\n",
    "\n",
    "display_images(images, shape=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 1.4**</span>. Use the *deep feedforward neural network* as the classifier to perform images classification task in a *single split training and testing*.\n",
    "\n",
    "The total marks for this question is <span style=\"color:red\">**[35 points]**</span>, with the following detailed breakdown sub-questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**(a)**</span> **Write your code to reshape the variable `x` storing features from `[num_samples, width, height]` dimension to `[num_samples, num_features]` with `num_features = width x height`. \n",
    "** (*Hint*: you might want to use the `reshape()` function)**<div style=\"text-align: right\"> <span style=\"color:red\">**[3 points]**</span> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "r,c = x[0].shape\n",
    "x = np.reshape(x,(y.shape[0],r*c))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training the DNN, scaling data is important. The pixel intensities of images are in the range of [0, 255], which makes the neural network difficult to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rescale the input data into the range of [0, 1]**\n",
    "<div style=\"text-align: right\"> \n",
    "<span style=\"color:red\">**[2 points]**</span> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "x_scale = MinMaxScaler().fit(x).transform(x)\n",
    "print(\"Min =\",format(x_scale.min()))\n",
    "print(\"Max =\",format(x_scale.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**(b)**</span> **Split the data into two subsets: 70% for training and 30% for testing. Note that you must use [*Stratified-Shuffle-Split*](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) to make sure training and testing are balanced and randomly shuffled before learning the model.**\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">**[5 points]**</span> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "ss_split=StratifiedShuffleSplit(train_size=0.7,random_state=1234)\n",
    "\n",
    "for train_index, test_index in ss_split.split(x_scale,y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#    print(str(train_index.shape)+\" \"+str(test_index.shape))\n",
    "    print(train_index.shape[0]+test_index.shape[0])\n",
    "    X_train, X_test = x_scale[train_index], x_scale[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**(c)**</span> **Construct a deep feedforward neural network with the following architecture:**\n",
    "\n",
    "* An input layer followed by *two* hidden layers, each with *500* hidden units, and an *output* layer;\n",
    "* *ReLU* activations for neurons in each hidden layer;\n",
    "* Training with gradient descent optimizer with learning rate **0.0011**, batch size 128 and 50 epochs.\n",
    "\n",
    "(*Hint*: *this question heavily relies on the knowledge you've learned from lab session in week 5 and 6. You are encouraged to revise these materials for this question*)\n",
    "\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">**[20 points]**</span> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL [5 marks]\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learn_rate = 0.0011\n",
    "num_inputs = r*c\n",
    "num_hidden1 = 500\n",
    "num_hidden2 = 500\n",
    "num_outputs = len(np.unique(y))\n",
    "\n",
    "inputs =  tf.placeholder(tf.float32,shape=[None, num_inputs],name='inputs')\n",
    "labels = tf.placeholder(tf.int64,shape=[None],name='labels')\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([num_inputs, num_outputs], stddev=0.02), name='weights')\n",
    "b = tf.Variable(tf.zeros([num_outputs]), name='biases')\n",
    "logits = tf.add(tf.matmul(inputs, W),b, name='logits')\n",
    "with tf.name_scope('evaluation'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learn_rate)\n",
    "    training_op=optimizer.minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL [3 marks]\n",
    "\n",
    "def neuron_layer(x, num_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        num_inputs=int(inputs.get_shape()[1])\n",
    "        std_dev=2/np.sqrt(num_inputs)\n",
    "        init=tf.truncated_normal([num_inputs,num_neurons],stddev=std_dev)\n",
    "        b = tf.Variable(tf.zeros([num_neurons]), name='biases')\n",
    "        W = tf.Variable(init,name='weights')\n",
    "        z = tf.matmul(inputs,W)+b\n",
    "    if activation == \"sigmoid\":\n",
    "        return tf.nn.sigmoid(z)\n",
    "    elif activation == \"relu\":\n",
    "        return tf.nn.relu(z)\n",
    "    else:\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL [7 marks]\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(num_inputs,num_hidden1, \"hidden1\", activation=\"relu\")\n",
    "    hidden2 = neuron_layer(hidden1, num_hidden2, \"hidden2\", activation=\"relu\")\n",
    "    logits = neuron_layer(hidden2, num_outputs, \"output\")\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"evaluation\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learn_rate)\n",
    "    grads=optimizer.compute_gradients(loss)\n",
    "    training_op=optimizer.apply_gradients(grads)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver=tf.train.Saver()\n",
    "    \n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name + \"/values\", var)\n",
    "        \n",
    "    for grad, var in grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name + \"/gradients\", grad)\n",
    "\n",
    "# summary\n",
    "accuracy_summary = tf.summary.scalar('accuracy',accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL [5 marks]\n",
    "\n",
    "# merge all summary\n",
    "tf.summary.histogram('hidden1/activations', hidden1)\n",
    "tf.summary.histogram('hidden2/activations', hidden2)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(\"models/dnn0/train\", tf.get_default_graph())\n",
    "test_writer = tf.summary.FileWriter(\"models/dnn0/test\", tf.get_default_graph())\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**(d)**</span> **You are now required write code to train the DNN.** Write codes in the following cell. <span style=\"color:red\">**[5 points]**</span> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    tbl = PrettyTable()\n",
    "\n",
    "    tbl.field_names = [\"Epoch\", \"Training Accuracy\", \"Testing Accuracy\"]\n",
    "    for epoch in range(num_epochs):\n",
    "        for idx_start in range(0, X_train.shape[0], batch_size):\n",
    "            idx_end = (idx_start+batch_size)\n",
    "            x_batch, y_batch = X_train[idx_start:idx_end,:],y_train[idx_start:idx_end,]\n",
    "            sess.run(training_op, feed_dict={inputs: x_batch, labels: y_batch})\n",
    "            \n",
    "        summary_train, acc_train = sess.run([merged,accuracy],feed_dict={inputs: x_batch, labels: y_batch})\n",
    "        summary_test, acc_test = sess.run([merged,accuracy],feed_dict={inputs: X_test, labels: y_test})\n",
    "        \n",
    "        train_writer.add_summary(summary_train, epoch)\n",
    "        test_writer.add_summary(summary_test, epoch)\n",
    "        tbl.add_row([epoch+1, np.round(acc_train,2),np.round(acc_test,2)])\n",
    "    print(tbl)\n",
    "    save_path = saver.save(sess, \"models/dnn0.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 2: Word2Vec, Text Analytics and Application</span>\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[Total mark for this part: 35 points]**<span></div>\n",
    "\n",
    "\n",
    "In this part, you are going to use Word2Vec for document classification on [20 Newsgroups](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html) dataset. This dataset is a collection of messages collected from 20 different netnews newsgroups. One thousand messages from each of the twenty newsgroups were chosen at random and partitioned by newsgroup name. The list of newsgroups from which the messages were chosen is as follows:\n",
    "\n",
    "`\n",
    "alt.atheism\n",
    "talk.politics.guns\n",
    "talk.politics.mideast\n",
    "talk.politics.misc\n",
    "talk.religion.misc\n",
    "soc.religion.christian\n",
    "comp.sys.ibm.pc.hardware\n",
    "comp.graphics\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\n",
    "rec.autos\n",
    "rec.motorcycles\n",
    "rec.sport.baseball\n",
    "rec.sport.hockey\n",
    "sci.crypt\n",
    "sci.electronics\n",
    "sci.space\n",
    "sci.med\n",
    "misc.forsale\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> Download the dataset and data pre-processing</span>\n",
    "\n",
    "####  <span style=\"color:red\">**Question 2.1**</span> Your first task is to run the following code to download the dataset.\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[1 point]**</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_all = fetch_20newsgroups(subset='all', remove=('headers'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 2.2**</span>. Print out the total number of documents, and the *unique* labels in this dataset.\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[1 point]**</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 2.3**</span>. Convert the documents into a list of tokens using the function `gensim.utils.tokenize`.\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[3 point]**</span></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> Train the model </span>\n",
    "\n",
    "\n",
    "####  <span style=\"color:red\">**Question 2.4**</span>. Train gensim's word2vec model.\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[5 points]**</span></div>\n",
    "\n",
    "* Train gensim's word2vec model with the settings of:\n",
    "    * The dimensionality of the feature vectors: `size=100`,\n",
    "    * The maximum distance between the current and predicted word within a sentence: `window=5`,\n",
    "    * Minimum frequence (ignore all words with total frequency lower than this): `min_count=5`,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save the trained model to a file named \"20_newsgroups.gensim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 2.5**</span>. Print out the vocabulary size (number of words in vocabulary).\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[2 points]**</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 2.6**</span>. Using the embedding results, calculate and print out the ten most similar words to word 'law' and their corresponding similarity scores.<div style=\"text-align: right\"><span style=\"color:red\">**[3 points]**</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> Evaluate the embeddings using classification </span>\n",
    "\n",
    "Now we investigate the quality of embedded vectors via document classification task. We have learned the embeddings for words, but not for documents yet, thus we need to find a way to extract the document embeddings from word embeddings. We are going to try two approaches:\n",
    "* Taking the **sum** of vectors of all words in the document; or\n",
    "* Taking the **average** of vectors of all words in the document.\n",
    "\n",
    "####  <span style=\"color:red\">**Question 2.7**</span>. Extract document vectors using `sum`.\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[5 points]**</span></div> \n",
    "\n",
    "* Remove all ***empty*** documents. A document is empty if it does not contain any word in the vocabulary;\n",
    "* Extract document vectors and save to variable `x`;\n",
    "* Save the corresponding labels to variable `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "\n",
    "def extract_doc2vec(doc, w2v):\n",
    "    vecs = [w2v[word] for word in doc if word in w2v]\n",
    "    if len(vecs) > 0:\n",
    "        vecs = np.asarray(vecs).sum(axis=0)\n",
    "    return vecs\n",
    "\n",
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 2.8**</span>. Print out the number of documents retained after removing empty documents.\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[1 point]**</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.9**</span>. Split the data into two subsets: 70% for training and 30% for testing. Note that you must use [*Stratified-Shuffle-Split*](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) to make sure training and testing are balanced and randomly shuffled before learning the model.\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">**[2 points]**</span> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.10**</span>. **Use [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) as the classifier, train and test the model using the training and test datasets from the previous step. Report the training accuracy and testing accuracy.**\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">**[2 points]**</span> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 2.11**</span>. Now modify the `extract_doc2vec` function above to  extract document vectors using `average`, instead of `sum`, and repeat the experiment: split the data, train and test using Logistic Regression.\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[5 points]**</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 2.12**</span>. Which representation (sum or average doc vector) gives the best performance? Write your observations and any lessons learned.\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">**[5 points]**</span></div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*[INSERT YOUR ANSWER HERE]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<div style=\"text-align: center\"> <span style=\"color:black\">**END OF ASSIGNMENT**</span> </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
